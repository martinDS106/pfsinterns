{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db144688",
   "metadata": {},
   "source": [
    "# Email Spam Detection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "7e999fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries \n",
    "import pandas as pd\n",
    "import string\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "68979bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('C:\\\\Users\\\\Martin\\\\Downloads\\\\spam.csv', encoding='ISO-8859-1')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89dab3d4",
   "metadata": {},
   "source": [
    "# Pre Processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9aa9652a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     v1                                                 v2\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop(columns=['Unnamed: 2','Unnamed: 3','Unnamed: 4'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d46245",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c98aaab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     v1                                                 v2\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee783f41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[2,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "170d3f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={'v1':'email_detection' ,'v2':'email_text' },inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2f06a858",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_email_text'] = df['email_text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e76b4265",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>email_detection</th>\n",
       "      <th>email_text</th>\n",
       "      <th>clean_email_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>go until jurong point, crazy.. available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>ok lar... joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>free entry in 2 a wkly comp to win fa cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>u dun say so early hor... u c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>nah i don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  email_detection                                         email_text  \\\n",
       "0             ham  Go until jurong point, crazy.. Available only ...   \n",
       "1             ham                      Ok lar... Joking wif u oni...   \n",
       "2            spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3             ham  U dun say so early hor... U c already then say...   \n",
       "4             ham  Nah I don't think he goes to usf, he lives aro...   \n",
       "\n",
       "                                    clean_email_text  \n",
       "0  go until jurong point, crazy.. available only ...  \n",
       "1                      ok lar... joking wif u oni...  \n",
       "2  free entry in 2 a wkly comp to win fa cup fina...  \n",
       "3  u dun say so early hor... u c already then say...  \n",
       "4  nah i don't think he goes to usf, he lives aro...  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb305052",
   "metadata": {},
   "source": [
    "# Remove Punctuations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5ce48d92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "99465a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuations(text):\n",
    "    return ''.join([char for char in text if char not in string.punctuation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "d85f788f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0e40f5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Remove_punctuation(text):\n",
    "    punctuation = string.punctuation \n",
    "    return text.translate(str.maketrans('','',punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ff9171ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>email_detection</th>\n",
       "      <th>email_text</th>\n",
       "      <th>clean_email_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>go until jurong point crazy available only in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>ok lar joking wif u oni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>free entry in 2 a wkly comp to win fa cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>u dun say so early hor u c already then say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>nah i dont think he goes to usf he lives aroun...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  email_detection                                         email_text  \\\n",
       "0             ham  Go until jurong point, crazy.. Available only ...   \n",
       "1             ham                      Ok lar... Joking wif u oni...   \n",
       "2            spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3             ham  U dun say so early hor... U c already then say...   \n",
       "4             ham  Nah I don't think he goes to usf, he lives aro...   \n",
       "\n",
       "                                    clean_email_text  \n",
       "0  go until jurong point crazy available only in ...  \n",
       "1                            ok lar joking wif u oni  \n",
       "2  free entry in 2 a wkly comp to win fa cup fina...  \n",
       "3        u dun say so early hor u c already then say  \n",
       "4  nah i dont think he goes to usf he lives aroun...  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['clean_email_text'] = df['clean_email_text'].apply(lambda x: remove_punctuations(x))\n",
    "# df['clean_email_text'] = df['clean_email_text'].apply(lambda x: Remove_punctuation(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dcd9b36",
   "metadata": {},
   "source": [
    "# Remove StopWrods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "212cb277",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"i, me, my, myself, we, our, ours, ourselves, you, you're, you've, you'll, you'd, your, yours, yourself, yourselves, he, him, his, himself, she, she's, her, hers, herself, it, it's, its, itself, they, them, their, theirs, themselves, what, which, who, whom, this, that, that'll, these, those, am, is, are, was, were, be, been, being, have, has, had, having, do, does, did, doing, a, an, the, and, but, if, or, because, as, until, while, of, at, by, for, with, about, against, between, into, through, during, before, after, above, below, to, from, up, down, in, out, on, off, over, under, again, further, then, once, here, there, when, where, why, how, all, any, both, each, few, more, most, other, some, such, no, nor, not, only, own, same, so, than, too, very, s, t, can, will, just, don, don't, should, should've, now, d, ll, m, o, re, ve, y, ain, aren, aren't, couldn, couldn't, didn, didn't, doesn, doesn't, hadn, hadn't, hasn, hasn't, haven, haven't, isn, isn't, ma, mightn, mightn't, mustn, mustn't, needn, needn't, shan, shan't, shouldn, shouldn't, wasn, wasn't, weren, weren't, won, won't, wouldn, wouldn't\""
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\", \".join(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8077b1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Stop_Words = set(stopwords.words('english'))\n",
    "def remove_stopwords(text):\n",
    "    return \" \".join([word for word in text.split() if word not in Stop_Words])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e5599e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_email_text'] = df['clean_email_text'].apply(lambda x : remove_stopwords(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff3d299",
   "metadata": {},
   "source": [
    "# Remove Duplicate words \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "33cca869",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicate_words(text):\n",
    "    words = text.split()\n",
    "    unique_words = set(words)\n",
    "    return ' '.join(unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5d3a11a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>email_detection</th>\n",
       "      <th>email_text</th>\n",
       "      <th>clean_email_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>amore world until bugis in jurong go n cine la...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>lar... ok oni... joking wif u</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>a 2 08452810075over18's final win in question(...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>already early say u then dun hor... so say... c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>don't here think he goes lives i around nah th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  email_detection                                         email_text  \\\n",
       "0             ham  Go until jurong point, crazy.. Available only ...   \n",
       "1             ham                      Ok lar... Joking wif u oni...   \n",
       "2            spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3             ham  U dun say so early hor... U c already then say...   \n",
       "4             ham  Nah I don't think he goes to usf, he lives aro...   \n",
       "\n",
       "                                    clean_email_text  \n",
       "0  amore world until bugis in jurong go n cine la...  \n",
       "1                      lar... ok oni... joking wif u  \n",
       "2  a 2 08452810075over18's final win in question(...  \n",
       "3    already early say u then dun hor... so say... c  \n",
       "4  don't here think he goes lives i around nah th...  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['clean_email_text'] = df['clean_email_text'].apply(lambda x : remove_duplicate_words(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb267c9d",
   "metadata": {},
   "source": [
    "# Remove Frequent words \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "805098c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('u', 1119),\n",
       " ('call', 576),\n",
       " ('2', 478),\n",
       " ('im', 462),\n",
       " ('get', 386),\n",
       " ('ur', 384),\n",
       " ('4', 287),\n",
       " ('dont', 279),\n",
       " ('go', 278),\n",
       " ('ok', 277),\n",
       " ('ltgt', 276),\n",
       " ('free', 275),\n",
       " ('know', 257),\n",
       " ('like', 242),\n",
       " ('got', 238),\n",
       " ('ill', 237),\n",
       " ('good', 234),\n",
       " ('come', 226),\n",
       " ('time', 208),\n",
       " ('day', 202),\n",
       " ('love', 195),\n",
       " ('want', 192),\n",
       " ('send', 190),\n",
       " ('text', 188),\n",
       " ('going', 171),\n",
       " ('one', 170),\n",
       " ('need', 166),\n",
       " ('txt', 163),\n",
       " ('home', 162),\n",
       " ('lor', 160),\n",
       " ('see', 156),\n",
       " ('sorry', 156),\n",
       " ('still', 153),\n",
       " ('r', 153),\n",
       " ('back', 152),\n",
       " ('stop', 150),\n",
       " ('reply', 144),\n",
       " ('n', 143),\n",
       " ('today', 141),\n",
       " ('mobile', 138),\n",
       " ('tell', 137),\n",
       " ('new', 136),\n",
       " ('later', 134),\n",
       " ('well', 133),\n",
       " ('think', 132),\n",
       " ('da', 131),\n",
       " ('hi', 131),\n",
       " ('please', 128),\n",
       " ('take', 126),\n",
       " ('phone', 125),\n",
       " ('cant', 123),\n",
       " ('ì', 117),\n",
       " ('week', 115),\n",
       " ('claim', 113),\n",
       " ('much', 113),\n",
       " ('night', 113),\n",
       " ('oh', 112),\n",
       " ('great', 111),\n",
       " ('hey', 111),\n",
       " ('dear', 110),\n",
       " ('pls', 109),\n",
       " ('happy', 106),\n",
       " ('hope', 103),\n",
       " ('make', 101),\n",
       " ('way', 101),\n",
       " ('work', 99),\n",
       " ('give', 99),\n",
       " ('wat', 96),\n",
       " ('thats', 94),\n",
       " ('number', 94),\n",
       " ('prize', 92),\n",
       " ('right', 92),\n",
       " ('say', 91),\n",
       " ('already', 90),\n",
       " ('tomorrow', 90),\n",
       " ('ask', 88),\n",
       " ('yes', 86),\n",
       " ('really', 86),\n",
       " ('yeah', 86),\n",
       " ('said', 86),\n",
       " ('e', 84),\n",
       " ('message', 83),\n",
       " ('1', 82),\n",
       " ('amp', 82),\n",
       " ('msg', 81),\n",
       " ('c', 79),\n",
       " ('didnt', 79),\n",
       " ('meet', 78),\n",
       " ('babe', 78),\n",
       " ('last', 78),\n",
       " ('morning', 77),\n",
       " ('life', 77),\n",
       " ('miss', 76),\n",
       " ('would', 76),\n",
       " ('cos', 76),\n",
       " ('ive', 74),\n",
       " ('cash', 74),\n",
       " ('thanks', 74),\n",
       " ('lol', 73),\n",
       " ('anything', 73)]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "word_count = Counter()\n",
    "for text in df['clean_email_text']:\n",
    "    for word in text.split():\n",
    "        word_count[word] += 1\n",
    "word_count.most_common(100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3ade021a",
   "metadata": {},
   "outputs": [],
   "source": [
    "FREQUENT_WORDS = set(word for word in word_count.most_common(10))\n",
    "def remove_freq_words(text):\n",
    "    return \" \".join([word for word in text.split() if word not in FREQUENT_WORDS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d293ff1b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>email_detection</th>\n",
       "      <th>email_text</th>\n",
       "      <th>clean_email_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>go jurong point crazy available bugis n great ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>ok lar joking wif u oni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>free entry 2 wkly comp win fa cup final tkts 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>u dun say early hor u c already say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>nah dont think goes usf lives around though</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>spam</td>\n",
       "      <td>FreeMsg Hey there darling it's been 3 week's n...</td>\n",
       "      <td>freemsg hey darling 3 weeks word back id like ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "      <td>even brother like speak treat like aids patent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ham</td>\n",
       "      <td>As per your request 'Melle Melle (Oru Minnamin...</td>\n",
       "      <td>per request melle melle oru minnaminunginte nu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>spam</td>\n",
       "      <td>WINNER!! As a valued network customer you have...</td>\n",
       "      <td>winner valued network customer selected receiv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>spam</td>\n",
       "      <td>Had your mobile 11 months or more? U R entitle...</td>\n",
       "      <td>mobile 11 months u r entitled update latest co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ham</td>\n",
       "      <td>I'm gonna be home soon and i don't want to tal...</td>\n",
       "      <td>im gonna home soon dont want talk stuff anymor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>spam</td>\n",
       "      <td>SIX chances to win CASH! From 100 to 20,000 po...</td>\n",
       "      <td>six chances win cash 100 20000 pounds txt csh1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>spam</td>\n",
       "      <td>URGENT! You have won a 1 week FREE membership ...</td>\n",
       "      <td>urgent 1 week free membership å£100000 prize j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ham</td>\n",
       "      <td>I've been searching for the right words to tha...</td>\n",
       "      <td>ive searching right words thank breather promi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "      <td>date sunday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>spam</td>\n",
       "      <td>XXXMobileMovieClub: To use your credit, click ...</td>\n",
       "      <td>xxxmobilemovieclub use credit click wap link n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ham</td>\n",
       "      <td>Oh k...i'm watching here:)</td>\n",
       "      <td>oh kim watching</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ham</td>\n",
       "      <td>Eh u remember how 2 spell his name... Yes i di...</td>\n",
       "      <td>eh u remember 2 spell name yes v naughty make ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ham</td>\n",
       "      <td>Fine if thatåÕs the way u feel. ThatåÕs the wa...</td>\n",
       "      <td>fine thatåõs way u feel thatåõs way gota b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>spam</td>\n",
       "      <td>England v Macedonia - dont miss the goals/team...</td>\n",
       "      <td>england v macedonia dont miss goalsteam news t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   email_detection                                         email_text  \\\n",
       "0              ham  Go until jurong point, crazy.. Available only ...   \n",
       "1              ham                      Ok lar... Joking wif u oni...   \n",
       "2             spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3              ham  U dun say so early hor... U c already then say...   \n",
       "4              ham  Nah I don't think he goes to usf, he lives aro...   \n",
       "5             spam  FreeMsg Hey there darling it's been 3 week's n...   \n",
       "6              ham  Even my brother is not like to speak with me. ...   \n",
       "7              ham  As per your request 'Melle Melle (Oru Minnamin...   \n",
       "8             spam  WINNER!! As a valued network customer you have...   \n",
       "9             spam  Had your mobile 11 months or more? U R entitle...   \n",
       "10             ham  I'm gonna be home soon and i don't want to tal...   \n",
       "11            spam  SIX chances to win CASH! From 100 to 20,000 po...   \n",
       "12            spam  URGENT! You have won a 1 week FREE membership ...   \n",
       "13             ham  I've been searching for the right words to tha...   \n",
       "14             ham                I HAVE A DATE ON SUNDAY WITH WILL!!   \n",
       "15            spam  XXXMobileMovieClub: To use your credit, click ...   \n",
       "16             ham                         Oh k...i'm watching here:)   \n",
       "17             ham  Eh u remember how 2 spell his name... Yes i di...   \n",
       "18             ham  Fine if thatåÕs the way u feel. ThatåÕs the wa...   \n",
       "19            spam  England v Macedonia - dont miss the goals/team...   \n",
       "\n",
       "                                     clean_email_text  \n",
       "0   go jurong point crazy available bugis n great ...  \n",
       "1                             ok lar joking wif u oni  \n",
       "2   free entry 2 wkly comp win fa cup final tkts 2...  \n",
       "3                 u dun say early hor u c already say  \n",
       "4         nah dont think goes usf lives around though  \n",
       "5   freemsg hey darling 3 weeks word back id like ...  \n",
       "6      even brother like speak treat like aids patent  \n",
       "7   per request melle melle oru minnaminunginte nu...  \n",
       "8   winner valued network customer selected receiv...  \n",
       "9   mobile 11 months u r entitled update latest co...  \n",
       "10  im gonna home soon dont want talk stuff anymor...  \n",
       "11  six chances win cash 100 20000 pounds txt csh1...  \n",
       "12  urgent 1 week free membership å£100000 prize j...  \n",
       "13  ive searching right words thank breather promi...  \n",
       "14                                        date sunday  \n",
       "15  xxxmobilemovieclub use credit click wap link n...  \n",
       "16                                    oh kim watching  \n",
       "17  eh u remember 2 spell name yes v naughty make ...  \n",
       "18         fine thatåõs way u feel thatåõs way gota b  \n",
       "19  england v macedonia dont miss goalsteam news t...  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['clean_email_text'] = df['clean_email_text'].apply(lambda x :remove_freq_words(x))\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "15e09da3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "618b921f",
   "metadata": {},
   "source": [
    "# Remove Rare words \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "06fc01c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('087187272008', 1),\n",
       " ('4years', 1),\n",
       " ('accent', 1),\n",
       " ('bitching', 1),\n",
       " ('dental', 1),\n",
       " ('dump', 1),\n",
       " ('heap', 1),\n",
       " ('kane', 1),\n",
       " ('lowes', 1),\n",
       " ('nmde', 1),\n",
       " ('now1', 1),\n",
       " ('pity', 1),\n",
       " ('pshewmissing', 1),\n",
       " ('salesman', 1),\n",
       " ('shud', 1),\n",
       " ('soany', 1),\n",
       " ('suggestions', 1),\n",
       " ('units', 1),\n",
       " ('å£750', 1)}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RARE_WORDS = set(word for word in word_count.most_common()[:-20:-1])\n",
    "RARE_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "01ec8a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Remove_Rare_Words(text):\n",
    "    return \" \".join(word for word in text.split() if word not in RARE_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "da19b84b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>email_detection</th>\n",
       "      <th>email_text</th>\n",
       "      <th>clean_email_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>go jurong point crazy available bugis n great ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>ok lar joking wif u oni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>free entry 2 wkly comp win fa cup final tkts 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>u dun say early hor u c already say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>nah dont think goes usf lives around though</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  email_detection                                         email_text  \\\n",
       "0             ham  Go until jurong point, crazy.. Available only ...   \n",
       "1             ham                      Ok lar... Joking wif u oni...   \n",
       "2            spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3             ham  U dun say so early hor... U c already then say...   \n",
       "4             ham  Nah I don't think he goes to usf, he lives aro...   \n",
       "\n",
       "                                    clean_email_text  \n",
       "0  go jurong point crazy available bugis n great ...  \n",
       "1                            ok lar joking wif u oni  \n",
       "2  free entry 2 wkly comp win fa cup final tkts 2...  \n",
       "3                u dun say early hor u c already say  \n",
       "4        nah dont think goes usf lives around though  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['clean_email_text'] = df['clean_email_text'].apply(lambda x : Remove_Rare_Words(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ffb5e2e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8       winner valued network customer selected receiv...\n",
       "9       mobile 11 months u r entitled update latest co...\n",
       "92      smile pleasure smile pain smile trouble pours ...\n",
       "98      hi wk ok hols yes bit run forgot hairdressers ...\n",
       "103     wow youre right didnt mean guess gave boston m...\n",
       "                              ...                        \n",
       "5462    txt call 86888 claim reward 3 hours talk time ...\n",
       "5484    picking various points going 2 yeovil motor pr...\n",
       "5486                           ofcourse also upload songs\n",
       "5544    im taking derek amp taylor walmart im back tim...\n",
       "5559                 arent next ltgt hours imma flip shit\n",
       "Name: clean_email_text, Length: 195, dtype: object"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lang = df['clean_email_text'].str.contains('our' , na=False)\n",
    "df.loc[lang,'clean_email_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588a6d1f",
   "metadata": {},
   "source": [
    "# Remove Specail Character  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "29a787af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d6e5496e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Remove_special_character(text):\n",
    "    text = re.sub('[^a-zA-Z0-9]',' ',text)\n",
    "    text = re.sub('\\s+',' ',text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c239e24a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>email_detection</th>\n",
       "      <th>email_text</th>\n",
       "      <th>clean_email_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>go jurong point crazy available bugis n great ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>ok lar joking wif u oni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>free entry 2 wkly comp win fa cup final tkts 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>u dun say early hor u c already say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>nah dont think goes usf lives around though</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>spam</td>\n",
       "      <td>FreeMsg Hey there darling it's been 3 week's n...</td>\n",
       "      <td>freemsg hey darling 3 weeks word back id like ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "      <td>even brother like speak treat like aids patent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ham</td>\n",
       "      <td>As per your request 'Melle Melle (Oru Minnamin...</td>\n",
       "      <td>per request melle melle oru minnaminunginte nu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>spam</td>\n",
       "      <td>WINNER!! As a valued network customer you have...</td>\n",
       "      <td>winner valued network customer selected receiv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>spam</td>\n",
       "      <td>Had your mobile 11 months or more? U R entitle...</td>\n",
       "      <td>mobile 11 months u r entitled update latest co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ham</td>\n",
       "      <td>I'm gonna be home soon and i don't want to tal...</td>\n",
       "      <td>im gonna home soon dont want talk stuff anymor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>spam</td>\n",
       "      <td>SIX chances to win CASH! From 100 to 20,000 po...</td>\n",
       "      <td>six chances win cash 100 20000 pounds txt csh1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>spam</td>\n",
       "      <td>URGENT! You have won a 1 week FREE membership ...</td>\n",
       "      <td>urgent 1 week free membership 100000 prize jac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ham</td>\n",
       "      <td>I've been searching for the right words to tha...</td>\n",
       "      <td>ive searching right words thank breather promi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "      <td>date sunday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>spam</td>\n",
       "      <td>XXXMobileMovieClub: To use your credit, click ...</td>\n",
       "      <td>xxxmobilemovieclub use credit click wap link n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ham</td>\n",
       "      <td>Oh k...i'm watching here:)</td>\n",
       "      <td>oh kim watching</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ham</td>\n",
       "      <td>Eh u remember how 2 spell his name... Yes i di...</td>\n",
       "      <td>eh u remember 2 spell name yes v naughty make ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ham</td>\n",
       "      <td>Fine if thatåÕs the way u feel. ThatåÕs the wa...</td>\n",
       "      <td>fine that s way u feel that s way gota b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>spam</td>\n",
       "      <td>England v Macedonia - dont miss the goals/team...</td>\n",
       "      <td>england v macedonia dont miss goalsteam news t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   email_detection                                         email_text  \\\n",
       "0              ham  Go until jurong point, crazy.. Available only ...   \n",
       "1              ham                      Ok lar... Joking wif u oni...   \n",
       "2             spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3              ham  U dun say so early hor... U c already then say...   \n",
       "4              ham  Nah I don't think he goes to usf, he lives aro...   \n",
       "5             spam  FreeMsg Hey there darling it's been 3 week's n...   \n",
       "6              ham  Even my brother is not like to speak with me. ...   \n",
       "7              ham  As per your request 'Melle Melle (Oru Minnamin...   \n",
       "8             spam  WINNER!! As a valued network customer you have...   \n",
       "9             spam  Had your mobile 11 months or more? U R entitle...   \n",
       "10             ham  I'm gonna be home soon and i don't want to tal...   \n",
       "11            spam  SIX chances to win CASH! From 100 to 20,000 po...   \n",
       "12            spam  URGENT! You have won a 1 week FREE membership ...   \n",
       "13             ham  I've been searching for the right words to tha...   \n",
       "14             ham                I HAVE A DATE ON SUNDAY WITH WILL!!   \n",
       "15            spam  XXXMobileMovieClub: To use your credit, click ...   \n",
       "16             ham                         Oh k...i'm watching here:)   \n",
       "17             ham  Eh u remember how 2 spell his name... Yes i di...   \n",
       "18             ham  Fine if thatåÕs the way u feel. ThatåÕs the wa...   \n",
       "19            spam  England v Macedonia - dont miss the goals/team...   \n",
       "\n",
       "                                     clean_email_text  \n",
       "0   go jurong point crazy available bugis n great ...  \n",
       "1                             ok lar joking wif u oni  \n",
       "2   free entry 2 wkly comp win fa cup final tkts 2...  \n",
       "3                 u dun say early hor u c already say  \n",
       "4         nah dont think goes usf lives around though  \n",
       "5   freemsg hey darling 3 weeks word back id like ...  \n",
       "6      even brother like speak treat like aids patent  \n",
       "7   per request melle melle oru minnaminunginte nu...  \n",
       "8   winner valued network customer selected receiv...  \n",
       "9   mobile 11 months u r entitled update latest co...  \n",
       "10  im gonna home soon dont want talk stuff anymor...  \n",
       "11  six chances win cash 100 20000 pounds txt csh1...  \n",
       "12  urgent 1 week free membership 100000 prize jac...  \n",
       "13  ive searching right words thank breather promi...  \n",
       "14                                        date sunday  \n",
       "15  xxxmobilemovieclub use credit click wap link n...  \n",
       "16                                    oh kim watching  \n",
       "17  eh u remember 2 spell name yes v naughty make ...  \n",
       "18           fine that s way u feel that s way gota b  \n",
       "19  england v macedonia dont miss goalsteam news t...  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['clean_email_text'] = df['clean_email_text'].apply(lambda x : Remove_special_character(x))\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0fd6360",
   "metadata": {},
   "source": [
    "# Lemmitization and POS tag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ef95adf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Martin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Martin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')  # Optional, for POS tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d07852e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Martin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c53bf025",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "860e5c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "wordnet_map = {\"N\":wordnet.NOUN, \"V\": wordnet.VERB, \"J\": wordnet.ADJ, \"R\": wordnet.ADV}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3b0723dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_words(text):\n",
    "    # find pos tags\n",
    "    pos_text = pos_tag(text.split())\n",
    "    return \" \".join([lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_text])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5250e7d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>email_detection</th>\n",
       "      <th>email_text</th>\n",
       "      <th>clean_email_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>go jurong point crazy available bugis n great ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>ok lar joking wif u oni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>free entry 2 wkly comp win fa cup final tkts 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>u dun say early hor u c already say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>nah dont think go usf life around though</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  email_detection                                         email_text  \\\n",
       "0             ham  Go until jurong point, crazy.. Available only ...   \n",
       "1             ham                      Ok lar... Joking wif u oni...   \n",
       "2            spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3             ham  U dun say so early hor... U c already then say...   \n",
       "4             ham  Nah I don't think he goes to usf, he lives aro...   \n",
       "\n",
       "                                    clean_email_text  \n",
       "0  go jurong point crazy available bugis n great ...  \n",
       "1                            ok lar joking wif u oni  \n",
       "2  free entry 2 wkly comp win fa cup final tkts 2...  \n",
       "3                u dun say early hor u c already say  \n",
       "4           nah dont think go usf life around though  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['clean_email_text'] = df['clean_email_text'].apply(lambda x: lemmatize_words(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b509224a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2f387d92",
   "metadata": {},
   "source": [
    "# Spelling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8b0417a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspellchecker in c:\\users\\martin\\anaconda3\\lib\\site-packages (0.8.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspellchecker "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817c4405",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['clean_email_text'] = df[''].apply(lambda x: Correct_spelling(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b52265ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spellchecker import SpellChecker\n",
    "spell = SpellChecker()\n",
    "\n",
    "def correct_spellings(text):\n",
    "    corrected_text = []\n",
    "    misspelled_text = spell.unknown(text.split())\n",
    "    # print(misspelled_text)\n",
    "    for word in text.split():\n",
    "        if word in misspelled_text:\n",
    "            correction = spell.correction(word)\n",
    "            corrected_text.append(correction if correction is not None else word)\n",
    "        else:\n",
    "            corrected_text.append(word)\n",
    "            \n",
    "    return \" \".join(corrected_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f1b8b02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_email_text'] = df['clean_email_text'].apply(lambda x: correct_spellings(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "38880aab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>email_detection</th>\n",
       "      <th>email_text</th>\n",
       "      <th>clean_email_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>go wrong point crazy available bugs n great wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>of lar joking if u on</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>free entry 2 wily come win fa cup final tits j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>u dun say early for u c already say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>nah don't think go us life around though</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  email_detection                                         email_text  \\\n",
       "0             ham  Go until jurong point, crazy.. Available only ...   \n",
       "1             ham                      Ok lar... Joking wif u oni...   \n",
       "2            spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3             ham  U dun say so early hor... U c already then say...   \n",
       "4             ham  Nah I don't think he goes to usf, he lives aro...   \n",
       "\n",
       "                                    clean_email_text  \n",
       "0  go wrong point crazy available bugs n great wo...  \n",
       "1                              of lar joking if u on  \n",
       "2  free entry 2 wily come win fa cup final tits j...  \n",
       "3                u dun say early for u c already say  \n",
       "4           nah don't think go us life around though  "
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85456fa1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "bdc3e598",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"4\" halign=\"left\">email_text</th>\n",
       "      <th colspan=\"4\" halign=\"left\">clean_email_text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>email_detection</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ham</th>\n",
       "      <td>4825</td>\n",
       "      <td>4516</td>\n",
       "      <td>Sorry, I'll call later</td>\n",
       "      <td>30</td>\n",
       "      <td>4825</td>\n",
       "      <td>4481</td>\n",
       "      <td>sorry ill call later</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spam</th>\n",
       "      <td>747</td>\n",
       "      <td>653</td>\n",
       "      <td>Please call our customer service representativ...</td>\n",
       "      <td>4</td>\n",
       "      <td>747</td>\n",
       "      <td>628</td>\n",
       "      <td>remember mobile 11mths entitled update late co...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                email_text         \\\n",
       "                     count unique   \n",
       "email_detection                     \n",
       "ham                   4825   4516   \n",
       "spam                   747    653   \n",
       "\n",
       "                                                                         \\\n",
       "                                                               top freq   \n",
       "email_detection                                                           \n",
       "ham                                         Sorry, I'll call later   30   \n",
       "spam             Please call our customer service representativ...    4   \n",
       "\n",
       "                clean_email_text         \\\n",
       "                           count unique   \n",
       "email_detection                           \n",
       "ham                         4825   4481   \n",
       "spam                         747    628   \n",
       "\n",
       "                                                                         \n",
       "                                                               top freq  \n",
       "email_detection                                                          \n",
       "ham                                           sorry ill call later   30  \n",
       "spam             remember mobile 11mths entitled update late co...    4  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('email_detection').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "fcc4a342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# turning spam/ham into numerical data\n",
    "df['email_detection'] = df['email_detection'].apply(lambda x : 1 if x == 'spam' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e05c22e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>email_detection</th>\n",
       "      <th>email_text</th>\n",
       "      <th>clean_email_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>go wrong point crazy available bugs n great wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>of lar joking if u on</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>free entry 2 wily come win fa cup final tits j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>u dun say early for u c already say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>nah don't think go us life around though</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   email_detection                                         email_text  \\\n",
       "0                0  Go until jurong point, crazy.. Available only ...   \n",
       "1                0                      Ok lar... Joking wif u oni...   \n",
       "2                1  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3                0  U dun say so early hor... U c already then say...   \n",
       "4                0  Nah I don't think he goes to usf, he lives aro...   \n",
       "\n",
       "                                    clean_email_text  \n",
       "0  go wrong point crazy available bugs n great wo...  \n",
       "1                              of lar joking if u on  \n",
       "2  free entry 2 wily come win fa cup final tits j...  \n",
       "3                u dun say early for u c already say  \n",
       "4           nah don't think go us life around though  "
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90119b4",
   "metadata": {},
   "source": [
    "#  Turning data into train and test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "11fec4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test ,y_train  , y_test= train_test_split(df.clean_email_text , df.email_detection , test_size = 0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672274ce",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a370659",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "84ca0418",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_count = cv.fit_transform(x_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "74c533d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4179x5774 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 34885 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "24cf9a74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_count.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41539b5e",
   "metadata": {},
   "source": [
    "# Train Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "0aca584c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "64fb7a11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train_count, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "1bff9c08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0], dtype=int64)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "email_ham = ['my name is martin from cairo']\n",
    "email_ham_count = cv.transform(email_ham)\n",
    "model.predict(email_ham_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "ea02fd78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1], dtype=int64)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "email_spam = ['reward']\n",
    "email_spam_count = cv.transform(email_spam)\n",
    "model.predict(email_spam_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f5f7ae",
   "metadata": {},
   "source": [
    "# Test Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "0735311d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9784637473079684"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test model\n",
    "x_test_count = cv.transform(x_test)\n",
    "model.score(x_test_count , y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec684a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
